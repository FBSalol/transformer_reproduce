# transformer_reproduce

目前self-attention得到了广泛的使用，该代码文件主要为了复现transformer的主要结构，对于训练和推理的部分进行省略，如果想了解这方面内容可以参阅：https://blog.csdn.net/m0_64148253/article/details/140422497